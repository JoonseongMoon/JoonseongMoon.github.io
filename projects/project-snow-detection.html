<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Roof Snow Detection — Joonseong</title>
<link href="https://fonts.googleapis.com/css2?family=DM+Serif+Display:ital@0;1&family=Instrument+Sans:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
<style>
  :root {
    --bg: #0a0a0a;
    --surface: #141414;
    --surface-hover: #1a1a1a;
    --border: #222;
    --border-light: #333;
    --text: #e8e6e1;
    --text-muted: #8a8780;
    --accent: #c4a882;
    --accent-dim: #8a7560;
    --accent-glow: rgba(196,168,130,0.08);
    --tag-bg: #1a1916;
    --tag-border: #2a2820;
    --code-bg: #111;
  }

  * { margin: 0; padding: 0; box-sizing: border-box; }
  html { scroll-behavior: smooth; scrollbar-width: thin; scrollbar-color: var(--border-light) transparent; }

  body {
    font-family: 'Instrument Sans', sans-serif;
    background: var(--bg);
    color: var(--text);
    line-height: 1.6;
    -webkit-font-smoothing: antialiased;
    overflow-x: hidden;
  }

  body::before {
    content: '';
    position: fixed;
    inset: 0;
    background-image: url("data:image/svg+xml,%3Csvg viewBox='0 0 256 256' xmlns='http://www.w3.org/2000/svg'%3E%3Cfilter id='n'%3E%3CfeTurbulence type='fractalNoise' baseFrequency='0.85' numOctaves='4' stitchTiles='stitch'/%3E%3C/filter%3E%3Crect width='100%25' height='100%25' filter='url(%23n)' opacity='0.04'/%3E%3C/svg%3E");
    pointer-events: none;
    z-index: 9999;
  }

  /* ─── Nav ─── */
  nav {
    position: fixed;
    top: 0;
    width: 100%;
    z-index: 100;
    padding: 1.25rem 3rem;
    display: flex;
    justify-content: space-between;
    align-items: center;
    background: rgba(10,10,10,0.8);
    backdrop-filter: blur(20px);
    border-bottom: 1px solid var(--border);
  }

  .nav-logo {
    font-family: 'DM Serif Display', serif;
    font-size: 1.3rem;
    color: var(--text);
    text-decoration: none;
    letter-spacing: -0.02em;
  }
  .nav-logo span { color: var(--accent); }

  .back-link {
    color: var(--text-muted);
    text-decoration: none;
    font-size: 0.82rem;
    font-weight: 500;
    letter-spacing: 0.08em;
    text-transform: uppercase;
    transition: color 0.3s;
    display: flex;
    align-items: center;
    gap: 0.5rem;
  }
  .back-link:hover { color: var(--accent); }

  /* ─── Project Hero ─── */
  .project-hero {
    padding: 10rem 3rem 4rem;
    max-width: 900px;
    margin: 0 auto;
    opacity: 0;
    animation: fadeUp 0.8s ease 0.2s forwards;
  }

  .project-hero-category {
    font-family: 'JetBrains Mono', monospace;
    font-size: 0.7rem;
    letter-spacing: 0.12em;
    text-transform: uppercase;
    color: var(--accent);
    margin-bottom: 1.25rem;
    display: flex;
    align-items: center;
    gap: 0.75rem;
  }

  .project-hero-category::before {
    content: '';
    width: 24px;
    height: 1px;
    background: var(--accent-dim);
  }

  .project-hero h1 {
    font-family: 'DM Serif Display', serif;
    font-size: clamp(2.2rem, 5vw, 3.5rem);
    font-weight: 400;
    line-height: 1.12;
    letter-spacing: -0.02em;
    margin-bottom: 1.5rem;
  }

  .project-hero-desc {
    font-size: 1.1rem;
    color: var(--text-muted);
    line-height: 1.8;
    max-width: 650px;
    margin-bottom: 2rem;
  }

  .project-meta-row {
    display: flex;
    flex-wrap: wrap;
    gap: 2rem;
    padding-top: 1.5rem;
    border-top: 1px solid var(--border);
  }

  .project-meta-item {
    display: flex;
    flex-direction: column;
    gap: 0.3rem;
  }

  .project-meta-item .label {
    font-size: 0.68rem;
    text-transform: uppercase;
    letter-spacing: 0.12em;
    color: var(--text-muted);
  }

  .project-meta-item .value {
    font-size: 0.88rem;
    color: var(--text);
  }

  .project-meta-item .value a {
    color: var(--accent);
    text-decoration: none;
    border-bottom: 1px solid transparent;
    transition: border-color 0.3s;
  }
  .project-meta-item .value a:hover { border-color: var(--accent); }

  /* ─── Cover Image ─── */
  .project-cover {
    width: 100%;
    max-width: 1100px;
    margin: 0 auto 4rem;
    padding: 0 3rem;
    opacity: 0;
    animation: fadeUp 0.8s ease 0.4s forwards;
  }

  .project-cover img, .project-cover .cover-placeholder {
    width: 100%;
    border-radius: 12px;
    border: 1px solid var(--border);
  }

  .cover-placeholder {
    height: 400px;
    background: linear-gradient(135deg, #1a1510, #0d1a18, #141018);
    display: flex;
    align-items: center;
    justify-content: center;
    font-family: 'JetBrains Mono', monospace;
    font-size: 0.75rem;
    color: var(--text-muted);
    letter-spacing: 0.1em;
    text-transform: uppercase;
  }

  /* ─── Content Area ─── */
  .project-content {
    max-width: 780px;
    margin: 0 auto;
    padding: 0 3rem 6rem;
  }

  /* ─── Section Blocks ─── */
  .content-section {
    margin-bottom: 4rem;
    opacity: 0;
    transform: translateY(20px);
    transition: all 0.6s ease;
  }

  .content-section.visible {
    opacity: 1;
    transform: translateY(0);
  }

  .content-section-label {
    font-family: 'JetBrains Mono', monospace;
    font-size: 0.68rem;
    letter-spacing: 0.12em;
    text-transform: uppercase;
    color: var(--accent-dim);
    margin-bottom: 1.25rem;
    display: flex;
    align-items: center;
    gap: 0.75rem;
  }

  .content-section-label::after {
    content: '';
    flex: 1;
    height: 1px;
    background: var(--border);
  }

  .content-section h2 {
    font-family: 'DM Serif Display', serif;
    font-size: 1.6rem;
    font-weight: 400;
    margin-bottom: 1rem;
    letter-spacing: -0.01em;
  }

  .content-section h3 {
    font-family: 'DM Serif Display', serif;
    font-size: 1.2rem;
    font-weight: 400;
    margin: 2rem 0 0.75rem;
    letter-spacing: -0.01em;
  }

  .content-section p {
    color: var(--text-muted);
    font-size: 0.95rem;
    line-height: 1.85;
    margin-bottom: 1rem;
  }

  .content-section p strong {
    color: var(--text);
    font-weight: 500;
  }

  /* ─── Image Gallery ─── */
  .image-gallery {
    display: grid;
    gap: 1rem;
    margin: 2rem 0;
  }

  .image-gallery.cols-2 { grid-template-columns: 1fr 1fr; }
  .image-gallery.cols-3 { grid-template-columns: 1fr 1fr 1fr; }
  .image-gallery.cols-1 { grid-template-columns: 1fr; }

  .gallery-item {
    position: relative;
    border-radius: 10px;
    overflow: hidden;
    border: 1px solid var(--border);
    background: #ffffff;
    display: flex;
    flex-direction: column;
    align-items: center;
  }

  .gallery-item img {
    max-width: 500px;
    width: 100%;
    object-fit: contain;
    display: block;
    padding: 1rem;
  }

  .gallery-caption {
    padding: 0.75rem 1rem;
    font-size: 0.78rem;
    color: var(--text-muted);
    border-top: 1px solid var(--border);
    width: 100%;
    background: var(--surface);
  }

  /* ─── Code Block ─── */
  .code-block {
    margin: 2rem 0;
    border-radius: 10px;
    border: 1px solid var(--border);
    overflow: hidden;
  }

  .code-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 0.65rem 1rem;
    background: var(--surface);
    border-bottom: 1px solid var(--border);
    font-family: 'JetBrains Mono', monospace;
    font-size: 0.72rem;
    color: var(--text-muted);
  }

  .code-dots {
    display: flex;
    gap: 6px;
  }
  .code-dots span {
    width: 8px;
    height: 8px;
    border-radius: 50%;
    background: var(--border-light);
  }

  .code-block pre {
    padding: 1.25rem;
    background: var(--code-bg);
    overflow-x: auto;
    font-family: 'JetBrains Mono', monospace;
    font-size: 0.82rem;
    line-height: 1.7;
    color: var(--text-muted);
  }

  .code-block pre .comment { color: #555; }
  .code-block pre .keyword { color: var(--accent); }
  .code-block pre .string { color: #7a9e7a; }
  .code-block pre .function { color: #8aabcc; }
  .code-block pre .number { color: #d4a76a; }

  /* ─── Process / Journey Timeline ─── */
  .timeline {
    margin: 2rem 0;
    position: relative;
    padding-left: 2rem;
  }

  .timeline::before {
    content: '';
    position: absolute;
    left: 3px;
    top: 8px;
    bottom: 8px;
    width: 1px;
    background: var(--border);
  }

  .timeline-item {
    position: relative;
    margin-bottom: 2rem;
    padding-left: 1.5rem;
  }

  .timeline-item:last-child { margin-bottom: 0; }

  .timeline-item::before {
    content: '';
    position: absolute;
    left: -2rem;
    top: 8px;
    width: 7px;
    height: 7px;
    border-radius: 50%;
    background: var(--accent-dim);
    border: 2px solid var(--bg);
    box-shadow: 0 0 0 1px var(--accent-dim);
  }

  .timeline-date {
    font-family: 'JetBrains Mono', monospace;
    font-size: 0.7rem;
    color: var(--accent-dim);
    margin-bottom: 0.35rem;
    letter-spacing: 0.05em;
  }

  .timeline-title {
    font-weight: 600;
    font-size: 0.95rem;
    margin-bottom: 0.35rem;
  }

  .timeline-desc {
    color: var(--text-muted);
    font-size: 0.85rem;
    line-height: 1.7;
  }

  /* ─── Key Findings / Highlights Box ─── */
  .highlight-box {
    margin: 2rem 0;
    padding: 1.5rem 1.75rem;
    background: var(--accent-glow);
    border: 1px solid var(--tag-border);
    border-left: 3px solid var(--accent-dim);
    border-radius: 0 10px 10px 0;
  }

  .highlight-box h4 {
    font-size: 0.85rem;
    font-weight: 600;
    margin-bottom: 0.75rem;
    color: var(--accent);
  }

  .highlight-box ul {
    list-style: none;
    padding: 0;
  }

  .highlight-box ul li {
    padding: 0.35rem 0;
    padding-left: 1.25rem;
    position: relative;
    color: var(--text-muted);
    font-size: 0.88rem;
    line-height: 1.6;
  }

  .highlight-box ul li::before {
    content: '→';
    position: absolute;
    left: 0;
    color: var(--accent-dim);
  }

  /* ─── Tags row ─── */
  .tags-row {
    display: flex;
    flex-wrap: wrap;
    gap: 0.4rem;
    margin: 1rem 0;
  }

  .tag {
    font-family: 'JetBrains Mono', monospace;
    font-size: 0.68rem;
    padding: 0.3rem 0.65rem;
    background: var(--tag-bg);
    border: 1px solid var(--tag-border);
    border-radius: 4px;
    color: var(--text-muted);
  }

  /* ─── Next / Prev Project Navigation ─── */
  .project-nav {
    max-width: 780px;
    margin: 0 auto;
    padding: 0 3rem 6rem;
    display: flex;
    justify-content: space-between;
    gap: 1rem;
  }

  .project-nav a {
    flex: 1;
    padding: 1.5rem;
    background: var(--surface);
    border: 1px solid var(--border);
    border-radius: 10px;
    text-decoration: none;
    transition: all 0.3s ease;
    display: flex;
    flex-direction: column;
    gap: 0.4rem;
  }

  .project-nav a:hover {
    border-color: var(--accent-dim);
    transform: translateY(-2px);
  }

  .project-nav .nav-direction {
    font-size: 0.68rem;
    text-transform: uppercase;
    letter-spacing: 0.1em;
    color: var(--text-muted);
  }

  .project-nav .nav-title {
    font-family: 'DM Serif Display', serif;
    font-size: 1.1rem;
    color: var(--text);
  }

  .project-nav a.next { text-align: right; }

  /* ─── Footer ─── */
  footer {
    padding: 2rem 3rem;
    border-top: 1px solid var(--border);
    display: flex;
    justify-content: space-between;
    align-items: center;
    font-size: 0.75rem;
    color: var(--text-muted);
  }

  @keyframes fadeUp {
    from { opacity: 0; transform: translateY(20px); }
    to { opacity: 1; transform: translateY(0); }
  }

  @media (max-width: 768px) {
    nav { padding: 1rem 1.5rem; }
    .project-hero { padding: 8rem 1.5rem 3rem; }
    .project-cover { padding: 0 1.5rem; }
    .project-content { padding: 0 1.5rem 4rem; }
    .project-meta-row { flex-direction: column; gap: 1rem; }
    .image-gallery.cols-2, .image-gallery.cols-3 { grid-template-columns: 1fr; }
    .project-nav { flex-direction: column; padding: 0 1.5rem 4rem; }
    footer { flex-direction: column; gap: 0.75rem; text-align: center; }
  }
</style>
</head>
<body>

<!-- ─── Nav ─── -->
<nav>
  <a href="../index.html" class="nav-logo">JS<span>.</span></a>
  <a href="../index.html" class="back-link">&larr; Back to Home</a>
</nav>

<!-- ─── Project Hero ─── -->
<div class="project-hero">
  <div class="project-hero-category">Computer Vision</div>
  <h1>Roof Snow Detection with U-Net</h1>
  <p class="project-hero-desc">
    A semantic segmentation pipeline for detecting snow coverage on drone-captured roof imagery, combining HSV-based automatic labeling with manual LabelMe annotations through a dual-loss U-Net architecture.
  </p>
  <div class="project-meta-row">
    <div class="project-meta-item">
      <span class="label">Type</span>
      <span class="value">Research Assistant</span>
    </div>
    <div class="project-meta-item">
      <span class="label">Year</span>
      <span class="value">2025</span>
    </div>
    <div class="project-meta-item">
      <span class="label">Role</span>
      <span class="value">Researcher / Developer</span>
    </div>
    <div class="project-meta-item">
      <span class="label">Stack</span>
      <span class="value">PyTorch, OpenCV, U-Net</span>
    </div>
  </div>
</div>

<!-- ─── Content ─── -->
<div class="project-content">

  <!-- Overview -->
  <div class="content-section">
    <div class="content-section-label">Overview</div>
    <h2>Problem Statement</h2>
    <p>
      Monitoring snow accumulation on building rooftops from drone imagery is critical for structural safety assessment. However, snow detection is challenging because <strong>bright snow</strong> (high value, low saturation) is easily captured by color thresholds, while <strong>shadow snow</strong> (snow in shaded areas) appears darker and is often misclassified as non-snow by simple HSV rules.
    </p>
    <p>
      This project develops a <strong>U-Net segmentation model</strong> that combines two complementary supervision signals: automated HSV-based masks for confident bright/dark pixels, and manual LabelMe polygon annotations for ambiguous shadow regions. A separated dual-loss function trains the network to handle both cases simultaneously.
    </p>
  </div>

  <!-- Pipeline -->
  <div class="content-section">
    <div class="content-section-label">Pipeline</div>
    <h2>Training Pipeline</h2>
    <div class="timeline">
      <div class="timeline-item">
        <div class="timeline-date">Stage 1 — HSV Auto-Labeling</div>
        <div class="timeline-title">Confidence-Based Mask Generation</div>
        <div class="timeline-desc">Converts images to HSV color space and classifies pixels into three categories: confident snow (V &ge; 200, S &le; 30), confident non-snow (V &lt; 100 or S &gt; 80), and uncertain (everything else, ignored during loss). Morphological cleanup removes noise.</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">Stage 2 — LabelMe Annotation</div>
        <div class="timeline-title">Manual Shadow Snow Labels</div>
        <div class="timeline-desc">Flagged images with ambiguous regions are manually annotated using LabelMe polygon tool. JSON annotations are parsed to binary masks, providing ground truth for shadow snow that HSV cannot capture.</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">Stage 3 — Roof Masking</div>
        <div class="timeline-title">Background Exclusion</div>
        <div class="timeline-desc">Black background pixels (from drone image cropping) are excluded via grayscale thresholding. All masks are intersected with the roof mask to ensure predictions are constrained to actual roof area.</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">Stage 4 — U-Net Training</div>
        <div class="timeline-title">Dual-Loss Optimization</div>
        <div class="timeline-desc">The model trains with a separated loss: HSV BCE loss on confident pixels + LabelMe BCE loss on annotated regions. Adam optimizer with ReduceLROnPlateau scheduler, 50 epochs, batch size 8, input size 256&times;256.</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">Stage 5 — Inference</div>
        <div class="timeline-title">Snow Coverage Estimation</div>
        <div class="timeline-desc">Trained model predicts pixel-level snow probability maps. Binary masks are generated at threshold 0.5, and snow coverage ratio is computed as snow pixels / roof pixels for each image.</div>
      </div>
    </div>
  </div>

  <!-- U-Net Architecture -->
  <div class="content-section">
    <div class="content-section-label">Architecture</div>
    <h2>U-Net Model</h2>
    <p>
      The model follows the classic <strong>U-Net encoder-decoder architecture</strong> with skip connections. The encoder has 4 stages (64 &rarr; 128 &rarr; 256 &rarr; 512 channels) with max pooling, a 1024-channel bottleneck, and a symmetric decoder with transposed convolutions. Each block uses <strong>DoubleConv</strong> (Conv2d &rarr; BatchNorm &rarr; ReLU &times; 2). Bilinear interpolation handles size mismatches at skip connections.
    </p>

    <div class="code-block">
      <div class="code-header">
        <div class="code-dots"><span></span><span></span><span></span></div>
        <span>unet.py</span>
      </div>
      <pre><span class="keyword">class</span> <span class="function">UNet</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, in_channels=<span class="number">3</span>, out_channels=<span class="number">1</span>):
        super().__init__()
        self.enc1 = DoubleConv(in_channels, <span class="number">64</span>)
        self.enc2 = DoubleConv(<span class="number">64</span>, <span class="number">128</span>)
        self.enc3 = DoubleConv(<span class="number">128</span>, <span class="number">256</span>)
        self.enc4 = DoubleConv(<span class="number">256</span>, <span class="number">512</span>)
        self.pool = nn.MaxPool2d(<span class="number">2</span>)
        self.bottleneck = DoubleConv(<span class="number">512</span>, <span class="number">1024</span>)

        <span class="comment"># Decoder with skip connections</span>
        self.up4 = nn.ConvTranspose2d(<span class="number">1024</span>, <span class="number">512</span>, <span class="number">2</span>, stride=<span class="number">2</span>)
        self.dec4 = DoubleConv(<span class="number">1024</span>, <span class="number">512</span>)
        <span class="comment">... # symmetric up2, up1</span>
        self.out = nn.Conv2d(<span class="number">64</span>, out_channels, <span class="number">1</span>)

    <span class="keyword">def</span> <span class="function">forward</span>(self, x):
        e1 = self.enc1(x)
        e2 = self.enc2(self.pool(e1))
        <span class="comment">... # encode → bottleneck → decode</span>
        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=<span class="number">1</span>))
        <span class="keyword">return</span> torch.sigmoid(self.out(d1))</pre>
    </div>
  </div>

  <!-- HSV Mask Generation -->
  <div class="content-section">
    <div class="content-section-label">Labeling</div>
    <h2>HSV Confidence Masking</h2>
    <p>
      Rather than treating all pixels equally, the HSV labeling pipeline creates <strong>three-class confidence masks</strong>. Only pixels where the color space provides high-confidence classification contribute to the HSV loss — uncertain pixels (moderate brightness and saturation) are ignored, preventing the model from learning noisy labels.
    </p>

    <div class="code-block">
      <div class="code-header">
        <div class="code-dots"><span></span><span></span><span></span></div>
        <span>hsv_labeling.py</span>
      </div>
      <pre><span class="keyword">def</span> <span class="function">get_hsv_mask_with_confidence</span>(image):
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    h, s, v = cv2.split(hsv)

    <span class="comment"># Confident SNOW: very bright AND very low saturation</span>
    hsv_positive = ((v &gt;= <span class="number">200</span>) &amp; (s &lt;= <span class="number">30</span>)).astype(np.uint8) * <span class="number">255</span>

    <span class="comment"># Confident NON-SNOW: dark OR colorful</span>
    hsv_negative = ((v &lt; <span class="number">100</span>) | (s &gt; <span class="number">80</span>)).astype(np.uint8) * <span class="number">255</span>

    <span class="comment"># Everything else is UNCERTAIN (ignored in loss)</span>
    hsv_ignore = <span class="number">255</span> - cv2.bitwise_or(hsv_positive, hsv_negative)

    <span class="comment"># Morphological cleanup</span>
    kernel = np.ones((<span class="number">5</span>, <span class="number">5</span>), np.uint8)
    hsv_positive = cv2.morphologyEx(hsv_positive, cv2.MORPH_OPEN, kernel)
    hsv_positive = cv2.morphologyEx(hsv_positive, cv2.MORPH_CLOSE, kernel)

    <span class="keyword">return</span> hsv_positive, hsv_negative, hsv_ignore</pre>
    </div>
  </div>

  <!-- Separated Loss -->
  <div class="content-section">
    <div class="content-section-label">Loss Function</div>
    <h2>Separated Dual-Loss</h2>
    <p>
      The key innovation is a <strong>separated loss function</strong> that independently weighs two supervision signals. The HSV loss is computed only on confident pixels (where HSV thresholding is reliable), while the LabelMe loss is computed only within manually annotated regions. This allows the model to learn from both automatic and manual labels without one corrupting the other.
    </p>

    <div class="code-block">
      <div class="code-header">
        <div class="code-dots"><span></span><span></span><span></span></div>
        <span>separated_loss.py</span>
      </div>
      <pre><span class="keyword">class</span> <span class="function">SeparatedSnowLoss</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">forward</span>(self, pred, hsv_snow_mask, hsv_valid_mask,
                labelme_mask, labelme_valid_mask):
        <span class="comment"># HSV Loss: only on confident HSV pixels</span>
        hsv_bce = F.binary_cross_entropy(pred, hsv_snow_mask, reduction=<span class="string">'none'</span>)
        hsv_loss = (hsv_bce * hsv_valid_mask).sum() / (hsv_valid_mask.sum() + <span class="number">1e-8</span>)

        <span class="comment"># LabelMe Loss: only on manually labeled pixels</span>
        lm_bce = F.binary_cross_entropy(pred, labelme_mask, reduction=<span class="string">'none'</span>)
        lm_loss = (lm_bce * labelme_valid_mask).sum() / (labelme_valid_mask.sum() + <span class="number">1e-8</span>)

        <span class="keyword">return</span> self.hsv_weight * hsv_loss + self.labelme_weight * lm_loss</pre>
    </div>
  </div>

  <!-- Data Pipeline -->
  <div class="content-section">
    <div class="content-section-label">Data</div>
    <h2>Dataset &amp; Augmentation</h2>
    <p>
      The custom <strong>SeparatedSnowDataset</strong> loads drone-captured roof crop images, generates HSV confidence masks on-the-fly, and optionally loads LabelMe JSON annotations for shadow snow regions. All masks are intersected with a roof mask (excluding black background) and resized with aspect-ratio-preserving padding to 256&times;256.
    </p>
    <p>
      Data augmentation includes random horizontal/vertical flips, 90/180/270-degree rotations, and brightness jittering (0.8&ndash;1.2&times;). Augmentations are applied identically to the image and all corresponding masks to maintain spatial alignment.
    </p>

    <div class="highlight-box">
      <h4>Dataset Returns (per image)</h4>
      <ul>
        <li><strong>image</strong>: RGB tensor (3 &times; 256 &times; 256)</li>
        <li><strong>hsv_snow_mask</strong>: Binary target for confident snow pixels</li>
        <li><strong>hsv_valid_mask</strong>: Where to compute HSV loss (snow + non-snow pixels)</li>
        <li><strong>labelme_mask</strong>: Binary target for manually annotated shadow snow</li>
        <li><strong>labelme_valid_mask</strong>: Dilated region around LabelMe annotations</li>
        <li><strong>roof_mask</strong>: Valid roof area (excludes black background)</li>
      </ul>
    </div>
  </div>

  <!-- Inference -->
  <div class="content-section">
    <div class="content-section-label">Inference</div>
    <h2>Prediction &amp; Output</h2>
    <p>
      At inference time, images are preprocessed with the same padding pipeline, passed through the trained U-Net to produce pixel-level probability maps, and thresholded at 0.5 to generate binary snow masks. The <strong>snow coverage ratio</strong> is calculated as the percentage of roof pixels classified as snow, and results are exported as ranked Excel spreadsheets alongside overlay visualizations.
    </p>

    <div class="highlight-box">
      <h4>Evaluation Metrics</h4>
      <ul>
        <li>IoU (Intersection over Union) computed separately for HSV and LabelMe regions</li>
        <li>Precision and Recall tracked per-image on validation set</li>
        <li>Training monitored with ReduceLROnPlateau scheduler (patience=5, factor=0.5)</li>
        <li>Best model saved based on combined validation loss</li>
      </ul>
    </div>
  </div>

  <!-- Tags -->
  <div class="content-section">
    <div class="content-section-label">Technologies</div>
    <div class="tags-row">
      <span class="tag">PyTorch</span>
      <span class="tag">U-Net</span>
      <span class="tag">OpenCV</span>
      <span class="tag">Semantic Segmentation</span>
      <span class="tag">HSV Color Space</span>
      <span class="tag">LabelMe</span>
      <span class="tag">Drone Imagery</span>
      <span class="tag">BatchNorm</span>
      <span class="tag">BCE Loss</span>
      <span class="tag">Scikit-learn</span>
    </div>
  </div>

</div>

<!-- ─── Next / Previous Project ─── -->
<div class="project-nav">
  <a href="project-energy-audit.html">
    <span class="nav-direction">&larr; Previous</span>
    <span class="nav-title">Smart Energy Audit</span>
  </a>
  <a href="project-digital-twin.html" class="next">
    <span class="nav-direction">Next &rarr;</span>
    <span class="nav-title">Spring-Mass-Damper Modeling</span>
  </a>
</div>

<footer>
  <span>&copy; 2025 Joonseong. Built with care.</span>
  <span>Pittsburgh, PA</span>
</footer>

<script>
  const observer = new IntersectionObserver((entries) => {
    entries.forEach(entry => {
      if (entry.isIntersecting) {
        entry.target.classList.add('visible');
        observer.unobserve(entry.target);
      }
    });
  }, { threshold: 0.1, rootMargin: '0px 0px -40px 0px' });
  document.querySelectorAll('.content-section').forEach(el => observer.observe(el));
</script>

</body>
</html>
